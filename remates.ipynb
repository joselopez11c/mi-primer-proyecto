{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joselopez11c/mi-primer-proyecto/blob/main/remates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbqjJG2omaDX"
      },
      "outputs": [],
      "source": [
        "NUMERO_REMATE_A_DETENER = '14039'   #COLOCAR SOLO EL NÚMERO SIN ESPACIOS\n",
        "\n",
        "!pip install pdfplumber\n",
        "!pip install openpyxl\n",
        "\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "from IPython.display import display, HTML\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "\n",
        "url = 'https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml'\n",
        "urlLogin = 'https://remaju.pj.gob.pe/remaju/pages/seguridad/login.xhtml'\n",
        "\n",
        "\n",
        "def initialize_session(url_login):\n",
        "  headers = {\n",
        "      'Accept': 'application/xml, text/xml, */*; q=0.01',\n",
        "      'Accept-Language': 'en-US,en;q=0.9,es;q=0.8',\n",
        "      'Connection': 'keep-alive',\n",
        "      'Content-Type': 'application/x-www-form-urlencoded',\n",
        "      'DNT': '1',\n",
        "      'Faces-Request': 'partial/ajax',\n",
        "      'Origin': 'https://remaju.pj.gob.pe',\n",
        "      'Referer': 'https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml',\n",
        "      'Sec-Fetch-Dest': 'empty',\n",
        "      'Sec-Fetch-Mode': 'cors',\n",
        "      'Sec-Fetch-Site': 'same-origin',\n",
        "      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
        "      'X-Requested-With': 'XMLHttpRequest',\n",
        "  }\n",
        "  response = requests.get(url_login, headers=headers)\n",
        "  headers['Cookie'] = 'JSESSIONID=' + response.cookies.get_dict().get('JSESSIONID')\n",
        "  return headers\n",
        "\n",
        "def fetch_page(headers, first_row):\n",
        "  dataPagination = {\n",
        "      'javax.faces.partial.ajax': 'true',\n",
        "      'javax.faces.source': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.partial.execute': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.partial.render': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.behavior.event': 'page',\n",
        "      'javax.faces.partial.event': 'page',\n",
        "      'formBuscarRemateExterno:listaRemate_pagination': 'true',\n",
        "      'formBuscarRemateExterno:listaRemate_skipChildren': 'true',\n",
        "      'formBuscarRemateExterno:listaRemate_first': str(first_row),\n",
        "      'formBuscarRemateExterno:listaRemate_rows': '12',\n",
        "      'formBuscarRemateExterno': 'formBuscarRemateExterno',\n",
        "  }\n",
        "  return requests.post(url, headers=headers, data=dataPagination)\n",
        "\n",
        "def downloadFile(cookie_header, idFile, view_state_value):\n",
        "  headers = {\n",
        "      'Accept': 'application/xml, text/xml, */*; q=0.01',\n",
        "      'Content-Type': 'application/x-www-form-urlencoded',\n",
        "      'Cookie': cookie_header\n",
        "  }\n",
        "\n",
        "  dataFile = {\n",
        "      'formBuscarRemateExterno': 'formBuscarRemateExterno',\n",
        "      idFile: '',\n",
        "      'javax.faces.ViewState': view_state_value\n",
        "  }\n",
        "\n",
        "  return requests.post(url, headers=headers, data=dataFile)\n",
        "\n",
        "\n",
        "def parse_page_content(page_content):\n",
        "  soup = BeautifulSoup(page_content, 'xml')\n",
        "  view_state_value = soup.find('update', {'id': 'j_id1:javax.faces.ViewState:0'}).text\n",
        "  cdata_content = soup.find('update').text\n",
        "  return BeautifulSoup(cdata_content, 'html.parser'), view_state_value\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "      for page in pdf.pages:\n",
        "          text += page.extract_text().replace('\\n', ' ') if page.extract_text() else ''\n",
        "  return text\n",
        "\n",
        "# Buscar todos los div con la clase 'card azul'\n",
        "def extract_card_data(soup_html, view_state_value, cookie_header):\n",
        "  cards = soup_html.find_all('div', class_='card azul')\n",
        "  data = []\n",
        "  for card in cards:\n",
        "    # Obtener número de remate y convocatoria\n",
        "    remate_info = card.find('span', class_='text-bold label-danger h6').get_text(strip=True) if card.find('span', class_='text-bold label-danger h6') else 'No disponible'\n",
        "\n",
        "    # Usar expresiones regulares para extraer solo el número de remate\n",
        "    match = re.search(r'Remate N° (\\d+)', remate_info)\n",
        "    if match:\n",
        "        remate_number = f\"N° {match.group(1)}\"  # Esto guardará \"N° 14066\"\n",
        "    else:\n",
        "        remate_number = 'No disponible'\n",
        "\n",
        "    # Dividir la información de remate en número y convocatoria, si es necesario\n",
        "    parts = remate_info.split(\" - \") if remate_info != 'No disponible' and len(remate_info.split(\" - \")) > 1 else ['No disponible', 'No disponible']\n",
        "    convocatoria = parts[1] if len(parts) > 1 else 'No disponible'\n",
        "\n",
        "    # Ubicación, buscando la etiqueta específica y extrayendo el texto siguiente\n",
        "    location_tag = card.find('i', class_='fa fa-map-marker')\n",
        "    location = location_tag.find_next_sibling(string=True).strip() if location_tag else 'No disponible'\n",
        "\n",
        "    # Descripción del inmueble, buscando el contenedor específico\n",
        "    description = card.find('div', class_='texto-info-scroll').get_text(strip=True) if card.find('div', class_='texto-info-scroll') else 'No disponible'\n",
        "\n",
        "    # Buscar todos los scripts dentro del card\n",
        "    scripts = card.find_all('script')\n",
        "    script_id = scripts[-1].get('id')[:-2] if scripts and scripts[-1].get('id').endswith('_s') else 'Script ID no encontrado'\n",
        "\n",
        "    # Guardar los datos extraídos en la lista\n",
        "    data.append({\n",
        "        'Remate': remate_number,\n",
        "        'Convocation': convocatoria,\n",
        "        'Location': location,\n",
        "        'Description': description,\n",
        "        'Script ID': script_id,\n",
        "        'View Code' : view_state_value,\n",
        "        'Cookie header' : cookie_header\n",
        "    })\n",
        "  return data\n",
        "\n",
        "def update_df_with_pdf_data(df, index, pdf_text, save_folder):\n",
        "  # Utilizar expresiones regulares para extraer la información del PDF\n",
        "  demandante_regex = re.search(r\"seguidos por (.*?), contra\", pdf_text, re.DOTALL)\n",
        "  demandado_regex = re.search(r\", contra (.*?), sobre\", pdf_text, re.DOTALL)\n",
        "  expediente_regex = re.search(r\"Expediente Judicial (.*?), tramitado\", pdf_text, re.DOTALL)\n",
        "  juzgado_regex = re.search(r\"tramitado ante el (.*?), a cargo\", pdf_text, re.DOTALL)\n",
        "\n",
        "  # Actualizar el DataFrame con los datos extraídos\n",
        "  df.at[index, 'Demandante'] = demandante_regex.group(1) if demandante_regex else \"No encontrado\"\n",
        "  df.at[index, 'Demandado'] = demandado_regex.group(1) if demandado_regex else \"No encontrado\"\n",
        "  df.at[index, 'Expediente_Judicial'] = expediente_regex.group(1) if expediente_regex else \"No encontrado\"\n",
        "  df.at[index, 'Juzgado'] = juzgado_regex.group(1).replace('\\n', ' ').strip() if juzgado_regex else \"No encontrado\"\n",
        "\n",
        "  # Adicionalmente, se puede realizar una acción específica si un criterio particular no se cumple\n",
        "  # Por ejemplo, eliminar el archivo PDF si el demandante no es el esperado\n",
        "  pdf_file_path = os.path.join(save_folder, f\"{df.at[index, 'Remate']}.pdf\")\n",
        "  if \"BANCO DE CREDITO DEL PERU\" not in (demandante_regex.group(1) if demandante_regex else \"\"):\n",
        "      if os.path.exists(pdf_file_path):\n",
        "          os.remove(pdf_file_path)\n",
        "\n",
        "\n",
        "def process_pdf_and_update_df(headers, df, save_folder):\n",
        "  for index, row in df.iterrows():\n",
        "      file_name = f\"{row['Remate']}.pdf\"\n",
        "      file_response = downloadFile(headers['Cookie'], row['Script ID'], row['View Code'])\n",
        "      if file_response.ok and 'application/pdf' in file_response.headers.get('Content-Type', ''):\n",
        "          pdf_path = os.path.join(save_folder, file_name)\n",
        "          with open(pdf_path, 'wb') as f:\n",
        "              f.write(file_response.content)\n",
        "          pdf_text = extract_pdf_data(pdf_path)\n",
        "          update_df_with_pdf_data(df, index, pdf_text, save_folder)  # Modified to pass save_folder\n",
        "      else:\n",
        "          print(\"Failed to download a valid PDF file.\")\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def process_and_save_data(df, save_folder, current_date):\n",
        "  df_filtered = df[df['Demandante'].str.contains(\"BANCO DE CREDITO DEL PERU\", na=False)]\n",
        "  columns_to_include = ['Remate', 'Demandante', 'Demandado', 'Expediente_Judicial', 'Juzgado']\n",
        "  df_to_save = df_filtered[columns_to_include]\n",
        "  excel_path = os.path.join(save_folder, f'Remates_Data_{current_date}.xlsx')\n",
        "  df_to_save.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "  return df_to_save\n",
        "\n",
        "\n",
        "def check_for_stop_condition(df, stop_number):\n",
        "  return any(df['Remate'] == stop_number)\n",
        "\n",
        "\n",
        "def print_drive_link(save_folder):\n",
        "  link = f\"https://drive.google.com/drive/u/0/search?q={save_folder.split('/')[-1]}\"\n",
        "  display(HTML(f\"<a href='{link}' target='_blank'>Haga clic aquí para ir al archivo guardado en Google Drive</a>\"))\n",
        "\n",
        "\n",
        "headers = initialize_session(urlLogin)\n",
        "current_date = datetime.datetime.now().strftime('%d_%m_%Y')\n",
        "\n",
        "save_folder = f'/content/drive/My Drive/remates_{current_date}'\n",
        "drive.mount('/content/drive')\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "stop_found = False\n",
        "first_row = 0\n",
        "count = 1\n",
        "stop_number = \"N° \" + NUMERO_REMATE_A_DETENER\n",
        "\n",
        "while not stop_found:\n",
        "    print(f\"Búscando en la página {count}\")\n",
        "    page_response = fetch_page(headers, first_row)\n",
        "    soup_html, view_state_value = parse_page_content(page_response.content)\n",
        "    page_data = extract_card_data(soup_html, view_state_value, headers['Cookie'])\n",
        "    page_df = pd.DataFrame(page_data)\n",
        "    df = pd.concat([df, page_df], ignore_index=True)\n",
        "\n",
        "    # Check if the stop condition is met\n",
        "    if check_for_stop_condition(page_df, stop_number):\n",
        "        print(f\"Se encontró el remate {stop_number}. Se detiene la búsqueda.\")\n",
        "        stop_found = True\n",
        "\n",
        "    # Preparar para la siguiente página\n",
        "    first_row += 12\n",
        "    count += 1\n",
        "\n",
        "# Continuar con el procesamiento de PDF y guardado de datos solo si se necesita\n",
        "if stop_found:\n",
        "    df = process_pdf_and_update_df(headers, df, save_folder)\n",
        "    df_to_save = process_and_save_data(df, save_folder, current_date)\n",
        "    print(\"Se terminó el proceso, hasta encontrar el remate: \"+ stop_number)\n",
        "    print_drive_link(save_folder)  # Display the clickable link\n",
        "else:\n",
        "    print(\"No se ha encontrado entre las paginas el número: \" + stop_number)\n",
        "\n",
        "\n",
        "df_to_save"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL9VGTkzpWJeUJGpK1PjV2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}