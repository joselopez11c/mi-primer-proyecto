{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joselopez11c/mi-primer-proyecto/blob/main/remates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbqjJG2omaDX",
        "outputId": "950bf023-2c5e-4d31-a198-ef6f3e0ba632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.0 pypdfium2-4.30.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install openpyxl\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO3oh__EoCgg"
      },
      "outputs": [],
      "source": [
        "url = 'https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml'\n",
        "urlLogin = 'https://remaju.pj.gob.pe/remaju/pages/seguridad/login.xhtml'\n",
        "\n",
        "headers = {\n",
        "    'Accept': 'application/xml, text/xml, */*; q=0.01',\n",
        "    'Accept-Language': 'en-US,en;q=0.9,es;q=0.8',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Content-Type': 'application/x-www-form-urlencoded',\n",
        "    'DNT': '1',\n",
        "    'Faces-Request': 'partial/ajax',\n",
        "    'Origin': 'https://remaju.pj.gob.pe',\n",
        "    'Referer': 'https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml',\n",
        "    'Sec-Fetch-Dest': 'empty',\n",
        "    'Sec-Fetch-Mode': 'cors',\n",
        "    'Sec-Fetch-Site': 'same-origin',\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "}\n",
        "\n",
        "response = requests.get(urlLogin, headers=headers)\n",
        "cookie_header = 'JSESSIONID=' + response.cookies.get_dict().get('JSESSIONID')\n",
        "headers['Cookie'] = cookie_header\n",
        "\n",
        "def fetch_page(first_row):\n",
        "  dataPagination = {\n",
        "      'javax.faces.partial.ajax': 'true',\n",
        "      'javax.faces.source': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.partial.execute': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.partial.render': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.behavior.event': 'page',\n",
        "      'javax.faces.partial.event': 'page',\n",
        "      'formBuscarRemateExterno:listaRemate_pagination': 'true',\n",
        "      'formBuscarRemateExterno:listaRemate_skipChildren': 'true',\n",
        "      'formBuscarRemateExterno:listaRemate_first': str(first_row),\n",
        "      'formBuscarRemateExterno:listaRemate_rows': '12',\n",
        "      'formBuscarRemateExterno': 'formBuscarRemateExterno',\n",
        "  }\n",
        "  response = requests.post(url, headers=headers, data=dataPagination)\n",
        "  return response\n",
        "\n",
        "def downloadFile(idFile):\n",
        "  headers = {\n",
        "      'Accept': 'application/xml, text/xml, */*; q=0.01',\n",
        "      'Content-Type': 'application/x-www-form-urlencoded',\n",
        "      'Cookie': cookie_header\n",
        "  }\n",
        "\n",
        "  dataFile = {\n",
        "      'formBuscarRemateExterno': 'formBuscarRemateExterno',\n",
        "      idFile: '',\n",
        "      'javax.faces.ViewState': view_state_value\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, headers=headers, data=dataFile)\n",
        "  return response\n",
        "\n",
        "\n",
        "page = fetch_page(24) ####################################################### LLAMADA A LA PAGINA\n",
        "\n",
        "def parse_page_content(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'xml')\n",
        "    view_state_value = soup.find('update', {'id': 'j_id1:javax.faces.ViewState:0'}).text\n",
        "    cdata_content = soup.find('update').text\n",
        "    return BeautifulSoup(cdata_content, 'html.parser'), view_state_value\n",
        "\n",
        "# Ahora parseamos el contenido del CDATA, que es HTML\n",
        "soup_html, view_state_value = parse_page_content(page.content)\n",
        "\n",
        "\n",
        "# Buscar todos los div con la clase 'card azul'\n",
        "def extract_card_data(soup_html, view_state_value, cookie_header):\n",
        "  cards = soup_html.find_all('div', class_='card azul')\n",
        "  data = []\n",
        "  for card in cards:\n",
        "      # Obtener número de remate y convocatoria\n",
        "      remate_info = card.find('span', class_='text-bold label-danger h6').get_text(strip=True) if card.find('span', class_='text-bold label-danger h6') else 'No disponible'\n",
        "\n",
        "      # Dividir la información de remate en número y convocatoria\n",
        "      if remate_info != 'No disponible':\n",
        "          parts = remate_info.split(\" - \")\n",
        "          remate_number = parts[0] if len(parts) > 0 else 'No disponible'\n",
        "          convocatoria = parts[1] if len(parts) > 1 else 'No disponible'\n",
        "      else:\n",
        "          remate_number = 'No disponible'\n",
        "          convocatoria = 'No disponible'\n",
        "\n",
        "      # Ubicación, buscando la etiqueta específica y extrayendo el texto siguiente\n",
        "      location_tag = card.find('i', class_='fa fa-map-marker')\n",
        "      location = location_tag.find_next_sibling(text=True).strip() if location_tag else 'No disponible'\n",
        "\n",
        "      # Descripción del inmueble, buscando el contenedor específico\n",
        "      description = card.find('div', class_='texto-info-scroll').get_text(strip=True) if card.find('div', class_='texto-info-scroll') else 'No disponible'\n",
        "\n",
        "      # Buscar todos los scripts dentro del card\n",
        "      scripts = card.find_all('script')\n",
        "      script_id = scripts[-1].get('id')[:-2] if scripts and scripts[-1].get('id').endswith('_s') else 'Script ID no encontrado'\n",
        "\n",
        "      # Guardar los datos extraídos en la lista\n",
        "      data.append({\n",
        "          'Numer': remate_number,\n",
        "          'Convocation': convocatoria,\n",
        "          'Location': location,\n",
        "          'Description': description,\n",
        "          'Script ID': script_id,\n",
        "          'View Code' : view_state_value,\n",
        "          'Cookie header' : cookie_header\n",
        "      })\n",
        "  return data\n",
        "\n",
        "data = extract_card_data(soup_html, view_state_value, headers['Cookie'])\n",
        "\n",
        "# Crear un DataFrame con los datos recolectados\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Demandante'] = \"\"\n",
        "df['Demandado'] = \"\"\n",
        "df['Expediente_Judicial'] = \"\"\n",
        "df['Juzgado'] = \"\"\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "save_folder = '/content/drive/My Drive/remates'\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    file_name = row['Numer'] + '.pdf'\n",
        "    response = downloadFile(row['Script ID'])\n",
        "\n",
        "    if response.ok and 'application/pdf' in response.headers.get('Content-Type', ''):\n",
        "        pdf_path = os.path.join(save_folder, file_name)\n",
        "        with open(pdf_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "              text += page.extract_text().replace('\\n', ' ') if page.extract_text() else ''\n",
        "\n",
        "        # Usar expresiones regulares para extraer la información\n",
        "        demandante = re.search(r\"seguidos por (.*?),\", text, re.DOTALL)\n",
        "        demandado = re.search(r\", contra (.*?), sobre\", text, re.DOTALL)\n",
        "        expediente = re.search(r\"Expediente Judicial (.*?),\", text, re.DOTALL)\n",
        "        juzgado = re.search(r\"tramitado ante el (.*?),\", text, re.DOTALL)\n",
        "\n",
        "        df.at[index, 'Demandante'] = demandante.group(1) if demandante else \"No encontrado\"\n",
        "        df.at[index, 'Demandado'] = demandado.group(1) if demandado else \"No encontrado\"\n",
        "        df.at[index, 'Expediente_Judicial'] = expediente.group(1) if expediente else \"No encontrado\"\n",
        "        df.at[index, 'Juzgado'] = juzgado.group(1).replace('\\n', ' ').strip() if juzgado else \"No encontrado\"\n",
        "\n",
        "        if \"BANCO DE CREDITO DEL PERU\" not in demandante.group(1) if demandante else \"\":\n",
        "            os.remove(pdf_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to download a valid PDF file.\")\n",
        "\n",
        "df = filtered_df = df[df['Demandante'].str.contains(\"BANCO DE CREDITO DEL PERU\", na=False)]\n",
        "\n",
        "columns_to_include = ['Numer', 'Demandante', 'Demandado', 'Expediente_Judicial', 'Juzgado']\n",
        "df_to_save = df[columns_to_include]\n",
        "\n",
        "excel_path = os.path.join(save_folder, 'Remates_Data.xlsx')\n",
        "\n",
        "# Usando openpyxl como motor para escribir archivos .xlsx\n",
        "df_to_save.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "df_to_save"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml'\n",
        "urlLogin = 'https://remaju.pj.gob.pe/remaju/pages/seguridad/login.xhtml'\n",
        "\n",
        "def initialize_session(url_login):\n",
        "  headers = {\n",
        "      'Accept': 'application/xml, text/xml, */*; q=0.01',\n",
        "      'Accept-Language': 'en-US,en;q=0.9,es;q=0.8',\n",
        "      'Connection': 'keep-alive',\n",
        "      'Content-Type': 'application/x-www-form-urlencoded',\n",
        "      'DNT': '1',\n",
        "      'Faces-Request': 'partial/ajax',\n",
        "      'Origin': 'https://remaju.pj.gob.pe',\n",
        "      'Referer': 'https://remaju.pj.gob.pe/remaju/pages/publico/remateExterno.xhtml',\n",
        "      'Sec-Fetch-Dest': 'empty',\n",
        "      'Sec-Fetch-Mode': 'cors',\n",
        "      'Sec-Fetch-Site': 'same-origin',\n",
        "      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
        "      'X-Requested-With': 'XMLHttpRequest',\n",
        "  }\n",
        "  response = requests.get(url_login, headers=headers)\n",
        "  headers['Cookie'] = 'JSESSIONID=' + response.cookies.get_dict().get('JSESSIONID')\n",
        "  return headers\n",
        "\n",
        "def fetch_page(headers, first_row):\n",
        "  dataPagination = {\n",
        "      'javax.faces.partial.ajax': 'true',\n",
        "      'javax.faces.source': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.partial.execute': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.partial.render': 'formBuscarRemateExterno:listaRemate',\n",
        "      'javax.faces.behavior.event': 'page',\n",
        "      'javax.faces.partial.event': 'page',\n",
        "      'formBuscarRemateExterno:listaRemate_pagination': 'true',\n",
        "      'formBuscarRemateExterno:listaRemate_skipChildren': 'true',\n",
        "      'formBuscarRemateExterno:listaRemate_first': str(first_row),\n",
        "      'formBuscarRemateExterno:listaRemate_rows': '12',\n",
        "      'formBuscarRemateExterno': 'formBuscarRemateExterno',\n",
        "  }\n",
        "  return requests.post(url, headers=headers, data=dataPagination)\n",
        "\n",
        "def downloadFile(cookie_header, idFile, view_state_value):\n",
        "  headers = {\n",
        "      'Accept': 'application/xml, text/xml, */*; q=0.01',\n",
        "      'Content-Type': 'application/x-www-form-urlencoded',\n",
        "      'Cookie': cookie_header\n",
        "  }\n",
        "\n",
        "  dataFile = {\n",
        "      'formBuscarRemateExterno': 'formBuscarRemateExterno',\n",
        "      idFile: '',\n",
        "      'javax.faces.ViewState': view_state_value\n",
        "  }\n",
        "\n",
        "  return requests.post(url, headers=headers, data=dataFile)\n",
        "\n",
        "\n",
        "def parse_page_content(page_content):\n",
        "  soup = BeautifulSoup(page_content, 'xml')\n",
        "  view_state_value = soup.find('update', {'id': 'j_id1:javax.faces.ViewState:0'}).text\n",
        "  cdata_content = soup.find('update').text\n",
        "  return BeautifulSoup(cdata_content, 'html.parser'), view_state_value\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "      for page in pdf.pages:\n",
        "          text += page.extract_text().replace('\\n', ' ') if page.extract_text() else ''\n",
        "  return text\n",
        "\n",
        "# Buscar todos los div con la clase 'card azul'\n",
        "def extract_card_data(soup_html, view_state_value, cookie_header):\n",
        "  cards = soup_html.find_all('div', class_='card azul')\n",
        "  data = []\n",
        "  for card in cards:\n",
        "    # Obtener número de remate y convocatoria\n",
        "    remate_info = card.find('span', class_='text-bold label-danger h6').get_text(strip=True) if card.find('span', class_='text-bold label-danger h6') else 'No disponible'\n",
        "\n",
        "    # Usar expresiones regulares para extraer solo el número de remate\n",
        "    match = re.search(r'Remate N° (\\d+)', remate_info)\n",
        "    if match:\n",
        "        remate_number = f\"N° {match.group(1)}\"  # Esto guardará \"N° 14066\"\n",
        "    else:\n",
        "        remate_number = 'No disponible'\n",
        "\n",
        "    # Dividir la información de remate en número y convocatoria, si es necesario\n",
        "    parts = remate_info.split(\" - \") if remate_info != 'No disponible' and len(remate_info.split(\" - \")) > 1 else ['No disponible', 'No disponible']\n",
        "    convocatoria = parts[1] if len(parts) > 1 else 'No disponible'\n",
        "\n",
        "    # Ubicación, buscando la etiqueta específica y extrayendo el texto siguiente\n",
        "    location_tag = card.find('i', class_='fa fa-map-marker')\n",
        "    location = location_tag.find_next_sibling(text=True).strip() if location_tag else 'No disponible'\n",
        "\n",
        "    # Descripción del inmueble, buscando el contenedor específico\n",
        "    description = card.find('div', class_='texto-info-scroll').get_text(strip=True) if card.find('div', class_='texto-info-scroll') else 'No disponible'\n",
        "\n",
        "    # Buscar todos los scripts dentro del card\n",
        "    scripts = card.find_all('script')\n",
        "    script_id = scripts[-1].get('id')[:-2] if scripts and scripts[-1].get('id').endswith('_s') else 'Script ID no encontrado'\n",
        "\n",
        "    # Guardar los datos extraídos en la lista\n",
        "    data.append({\n",
        "        'Number': remate_number,\n",
        "        'Convocation': convocatoria,\n",
        "        'Location': location,\n",
        "        'Description': description,\n",
        "        'Script ID': script_id,\n",
        "        'View Code' : view_state_value,\n",
        "        'Cookie header' : cookie_header\n",
        "    })\n",
        "  return data\n",
        "\n",
        "def update_df_with_pdf_data(df, index, pdf_text):\n",
        "  # Utilizar expresiones regulares para extraer la información del PDF\n",
        "  demandante_regex = re.search(r\"seguidos por (.*?), contra\", pdf_text, re.DOTALL)\n",
        "  demandado_regex = re.search(r\", contra (.*?), sobre\", pdf_text, re.DOTALL)\n",
        "  expediente_regex = re.search(r\"Expediente Judicial (.*?), tramitado\", pdf_text, re.DOTALL)\n",
        "  juzgado_regex = re.search(r\"tramitado ante el (.*?), a cargo\", pdf_text, re.DOTALL)\n",
        "\n",
        "  # Actualizar el DataFrame con los datos extraídos\n",
        "  df.at[index, 'Demandante'] = demandante_regex.group(1) if demandante_regex else \"No encontrado\"\n",
        "  df.at[index, 'Demandado'] = demandado_regex.group(1) if demandado_regex else \"No encontrado\"\n",
        "  df.at[index, 'Expediente_Judicial'] = expediente_regex.group(1) if expediente_regex else \"No encontrado\"\n",
        "  df.at[index, 'Juzgado'] = juzgado_regex.group(1).replace('\\n', ' ').strip() if juzgado_regex else \"No encontrado\"\n",
        "\n",
        "  # Adicionalmente, se puede realizar una acción específica si un criterio particular no se cumple\n",
        "  # Por ejemplo, eliminar el archivo PDF si el demandante no es el esperado\n",
        "  if \"BANCO DE CREDITO DEL PERU\" not in (demandante_regex.group(1) if demandante_regex else \"\"):\n",
        "      os.remove(os.path.join('/content/drive/My Drive/remates', f\"{df.at[index, 'Number']}.pdf\"))\n",
        "\n",
        "def process_pdf_and_update_df(headers, df, save_folder):\n",
        "  for index, row in df.iterrows():\n",
        "      file_name = f\"{row['Number']}.pdf\"\n",
        "      file_response = downloadFile(headers['Cookie'], row['Script ID'], row['View Code'])\n",
        "      if file_response.ok and 'application/pdf' in file_response.headers.get('Content-Type', ''):\n",
        "          pdf_path = os.path.join(save_folder, file_name)\n",
        "          with open(pdf_path, 'wb') as f:\n",
        "              f.write(file_response.content)\n",
        "          pdf_text = extract_pdf_data(pdf_path)\n",
        "          update_df_with_pdf_data(df, index, pdf_text)\n",
        "      else:\n",
        "          print(\"Failed to download a valid PDF file.\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def process_and_save_data(df, save_folder):\n",
        "  #print('Demandante: ' + df['Demandante'])\n",
        "  df_filtered = df[df['Demandante'].str.contains(\"BANCO DE CREDITO DEL PERU\", na=False)]\n",
        "  columns_to_include = ['Number', 'Demandante', 'Demandado', 'Expediente_Judicial', 'Juzgado']\n",
        "  df_to_save = df_filtered[columns_to_include]\n",
        "  excel_path = os.path.join(save_folder, 'Remates_Data.xlsx')\n",
        "  df_to_save.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "  return df_to_save\n",
        "\n",
        "\n",
        "def check_for_stop_condition(df, stop_number):\n",
        "  return any(df['Number'] == stop_number)\n",
        "\n",
        "\n",
        "stop_number = \"N° 14039\"  # Aquí defines el número en el que quieres detener la búsqueda\n",
        "\n",
        "headers = initialize_session(urlLogin)\n",
        "save_folder = '/content/drive/My Drive/remates'\n",
        "drive.mount('/content/drive')\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "stop_found = False\n",
        "first_row = 0\n",
        "\n",
        "while not stop_found:\n",
        "    print(f\"Fetching page starting at row {first_row}\")\n",
        "    page_response = fetch_page(headers, first_row)\n",
        "    soup_html, view_state_value = parse_page_content(page_response.content)\n",
        "    page_data = extract_card_data(soup_html, view_state_value, headers['Cookie'])\n",
        "    page_df = pd.DataFrame(page_data)\n",
        "    df = pd.concat([df, page_df], ignore_index=True)\n",
        "\n",
        "    # Check if the stop condition is met\n",
        "    if check_for_stop_condition(page_df, stop_number):\n",
        "        print(f\"Stop number {stop_number} found. Stopping the search.\")\n",
        "        stop_found = True\n",
        "\n",
        "    # Preparar para la siguiente página\n",
        "    first_row += 12\n",
        "\n",
        "# Continuar con el procesamiento de PDF y guardado de datos solo si se necesita\n",
        "if stop_found:\n",
        "    df = process_pdf_and_update_df(headers, df, save_folder)\n",
        "    df_to_save = process_and_save_data(df, save_folder)\n",
        "    df_to_save\n",
        "else:\n",
        "    print(\"The specified stop number was not found in the pages fetched.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU0fMmNFyCHP",
        "outputId": "d891977f-cdd9-45fc-d3ac-b23adc51f2ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fetching page starting at row 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-4af213e60fd3>:90: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  location = location_tag.find_next_sibling(text=True).strip() if location_tag else 'No disponible'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page starting at row 12\n",
            "Fetching page starting at row 24\n",
            "Stop number N° 14039 found. Stopping the search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9bIHDqh0lcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpIi0OmfP8/+JM5Uj0wtiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}